{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKOgSPO_Zft6"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('max_columns', None)\n",
        "pd.set_option('max_rows', 200)\n",
        "pd.set_option('float_format', lambda x: '%.3f' % x)\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "81dk7UtvgfEa",
        "outputId": "508e5f58-2b82-4637-d37a-2b6c824ab96c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loan_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>total_loan</th>\n",
              "      <th>year_of_loan</th>\n",
              "      <th>interest</th>\n",
              "      <th>monthly_payment</th>\n",
              "      <th>class</th>\n",
              "      <th>employer_type</th>\n",
              "      <th>industry</th>\n",
              "      <th>work_year</th>\n",
              "      <th>house_exist</th>\n",
              "      <th>censor_status</th>\n",
              "      <th>issue_date</th>\n",
              "      <th>use</th>\n",
              "      <th>post_code</th>\n",
              "      <th>region</th>\n",
              "      <th>debt_loan_ratio</th>\n",
              "      <th>del_in_18month</th>\n",
              "      <th>scoring_low</th>\n",
              "      <th>scoring_high</th>\n",
              "      <th>known_outstanding_loan</th>\n",
              "      <th>known_dero</th>\n",
              "      <th>pub_dero_bankrup</th>\n",
              "      <th>recircle_b</th>\n",
              "      <th>recircle_u</th>\n",
              "      <th>initial_list_status</th>\n",
              "      <th>app_type</th>\n",
              "      <th>earlies_credit_mon</th>\n",
              "      <th>title</th>\n",
              "      <th>policy_code</th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>early_return</th>\n",
              "      <th>early_return_amount</th>\n",
              "      <th>early_return_amount_3mon</th>\n",
              "      <th>isDefault</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1040418</td>\n",
              "      <td>240418</td>\n",
              "      <td>31818.182</td>\n",
              "      <td>3</td>\n",
              "      <td>11.466</td>\n",
              "      <td>1174.910</td>\n",
              "      <td>C</td>\n",
              "      <td>政府机构</td>\n",
              "      <td>金融业</td>\n",
              "      <td>3 years</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2016/10/1</td>\n",
              "      <td>2</td>\n",
              "      <td>193</td>\n",
              "      <td>13</td>\n",
              "      <td>2.430</td>\n",
              "      <td>0</td>\n",
              "      <td>556.364</td>\n",
              "      <td>649.091</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>7734.231</td>\n",
              "      <td>91.800</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1-Dec</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4.000</td>\n",
              "      <td>5.000</td>\n",
              "      <td>4.000</td>\n",
              "      <td>3</td>\n",
              "      <td>9927</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1025197</td>\n",
              "      <td>225197</td>\n",
              "      <td>28000.000</td>\n",
              "      <td>5</td>\n",
              "      <td>16.841</td>\n",
              "      <td>670.690</td>\n",
              "      <td>C</td>\n",
              "      <td>政府机构</td>\n",
              "      <td>金融业</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2013/6/1</td>\n",
              "      <td>0</td>\n",
              "      <td>491</td>\n",
              "      <td>30</td>\n",
              "      <td>11.005</td>\n",
              "      <td>1</td>\n",
              "      <td>715.000</td>\n",
              "      <td>893.750</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>31329.000</td>\n",
              "      <td>54.800</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Apr-90</td>\n",
              "      <td>40642</td>\n",
              "      <td>1</td>\n",
              "      <td>7.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4.000</td>\n",
              "      <td>45.000</td>\n",
              "      <td>22.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1009360</td>\n",
              "      <td>209360</td>\n",
              "      <td>17272.727</td>\n",
              "      <td>3</td>\n",
              "      <td>8.900</td>\n",
              "      <td>603.320</td>\n",
              "      <td>A</td>\n",
              "      <td>政府机构</td>\n",
              "      <td>公共服务、社会组织</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2014/1/1</td>\n",
              "      <td>4</td>\n",
              "      <td>459</td>\n",
              "      <td>8</td>\n",
              "      <td>6.409</td>\n",
              "      <td>0</td>\n",
              "      <td>774.545</td>\n",
              "      <td>903.636</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>18514.000</td>\n",
              "      <td>57.692</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Oct-91</td>\n",
              "      <td>154</td>\n",
              "      <td>1</td>\n",
              "      <td>6.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6.000</td>\n",
              "      <td>28.000</td>\n",
              "      <td>19.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1039708</td>\n",
              "      <td>239708</td>\n",
              "      <td>20000.000</td>\n",
              "      <td>3</td>\n",
              "      <td>4.788</td>\n",
              "      <td>602.300</td>\n",
              "      <td>A</td>\n",
              "      <td>世界五百强</td>\n",
              "      <td>文化和体育业</td>\n",
              "      <td>6 years</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2015/7/1</td>\n",
              "      <td>0</td>\n",
              "      <td>157</td>\n",
              "      <td>8</td>\n",
              "      <td>9.205</td>\n",
              "      <td>0</td>\n",
              "      <td>750.000</td>\n",
              "      <td>875.000</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>20707.000</td>\n",
              "      <td>42.600</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1-Jun</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>10.000</td>\n",
              "      <td>15.000</td>\n",
              "      <td>9.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1027483</td>\n",
              "      <td>227483</td>\n",
              "      <td>15272.727</td>\n",
              "      <td>3</td>\n",
              "      <td>12.790</td>\n",
              "      <td>470.310</td>\n",
              "      <td>C</td>\n",
              "      <td>政府机构</td>\n",
              "      <td>信息传输、软件和信息技术服务业</td>\n",
              "      <td>&lt; 1 year</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2016/7/1</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>21</td>\n",
              "      <td>15.578</td>\n",
              "      <td>0</td>\n",
              "      <td>609.091</td>\n",
              "      <td>710.606</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>14016.154</td>\n",
              "      <td>30.462</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2-May</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6.000</td>\n",
              "      <td>15.000</td>\n",
              "      <td>4.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loan_id  user_id  total_loan  year_of_loan  interest  monthly_payment  \\\n",
              "0  1040418   240418   31818.182             3    11.466         1174.910   \n",
              "1  1025197   225197   28000.000             5    16.841          670.690   \n",
              "2  1009360   209360   17272.727             3     8.900          603.320   \n",
              "3  1039708   239708   20000.000             3     4.788          602.300   \n",
              "4  1027483   227483   15272.727             3    12.790          470.310   \n",
              "\n",
              "  class employer_type         industry  work_year  house_exist  censor_status  \\\n",
              "0     C          政府机构              金融业    3 years            0              1   \n",
              "1     C          政府机构              金融业  10+ years            0              2   \n",
              "2     A          政府机构        公共服务、社会组织  10+ years            1              0   \n",
              "3     A         世界五百强           文化和体育业    6 years            0              1   \n",
              "4     C          政府机构  信息传输、软件和信息技术服务业   < 1 year            2              1   \n",
              "\n",
              "  issue_date  use  post_code  region  debt_loan_ratio  del_in_18month  \\\n",
              "0  2016/10/1    2        193      13            2.430               0   \n",
              "1   2013/6/1    0        491      30           11.005               1   \n",
              "2   2014/1/1    4        459       8            6.409               0   \n",
              "3   2015/7/1    0        157       8            9.205               0   \n",
              "4   2016/7/1    0         38      21           15.578               0   \n",
              "\n",
              "   scoring_low  scoring_high  known_outstanding_loan  known_dero  \\\n",
              "0      556.364       649.091                       3           0   \n",
              "1      715.000       893.750                       3           0   \n",
              "2      774.545       903.636                       5           0   \n",
              "3      750.000       875.000                       3           0   \n",
              "4      609.091       710.606                      15           0   \n",
              "\n",
              "   pub_dero_bankrup  recircle_b  recircle_u  initial_list_status  app_type  \\\n",
              "0             0.000    7734.231      91.800                    0         0   \n",
              "1             0.000   31329.000      54.800                    1         0   \n",
              "2             0.000   18514.000      57.692                    1         0   \n",
              "3             0.000   20707.000      42.600                    0         0   \n",
              "4             0.000   14016.154      30.462                    0         0   \n",
              "\n",
              "  earlies_credit_mon  title  policy_code     f0    f1     f2     f3     f4  \\\n",
              "0              1-Dec      5            1  1.000 0.000  4.000  5.000  4.000   \n",
              "1             Apr-90  40642            1  7.000 0.000  4.000 45.000 22.000   \n",
              "2             Oct-91    154            1  6.000 0.000  6.000 28.000 19.000   \n",
              "3              1-Jun      0            1  5.000 0.000 10.000 15.000  9.000   \n",
              "4              2-May      0            1 10.000 0.000  6.000 15.000  4.000   \n",
              "\n",
              "   early_return  early_return_amount  early_return_amount_3mon  isDefault  \n",
              "0             3                 9927                     0.000          0  \n",
              "1             0                    0                     0.000          0  \n",
              "2             0                    0                     0.000          0  \n",
              "3             0                    0                     0.000          0  \n",
              "4             0                    0                     0.000          0  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_bank = pd.read_csv('/content/drive/MyDrive/CCF-contest/Personal_loan_default_forecast/train_public.csv')\n",
        "train_internet = pd.read_csv('/content/drive/MyDrive/CCF-contest/Personal_loan_default_forecast/train_internet.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/CCF-contest/Personal_loan_default_forecast/test_public.csv')\n",
        "train_bank.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25voQojEgfEm"
      },
      "source": [
        "### 数据预处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AVzhPK6MP2S"
      },
      "outputs": [],
      "source": [
        "train_internet = train_internet.rename(columns={'is_default': 'isDefault'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCaZZFEKgfEq",
        "outputId": "abcbb3fe-d56a-4e04-9a08-4fb3d18c59f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "common_cols = []\n",
        "for col in train_bank.columns:\n",
        "    if col in train_internet.columns:\n",
        "        common_cols.append(col)\n",
        "    else: continue\n",
        "len(common_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjHYnRBagfEt",
        "outputId": "2d9c8ddb-c0ef-452d-a7bd-2ff96e629e5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39\n",
            "42\n"
          ]
        }
      ],
      "source": [
        "print(len(train_bank.columns))\n",
        "print(len(train_internet.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbEi9fAKgfEt",
        "outputId": "29d9db05-8ef6-494a-b4a2-7d295f1a6b8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['known_dero', 'known_outstanding_loan', 'app_type']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_bank_left = list(set(list(train_bank.columns)) - set(common_cols))\n",
        "train_internet_left = list(set(list(train_internet.columns)) - set(common_cols))\n",
        "\n",
        "train_bank_left"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ls19ruigfEv",
        "outputId": "1145b870-7f42-459f-c6ae-3e5ce9fb7d57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['sub_class', 'marriage', 'work_type', 'offsprings', 'f5', 'house_loan_status']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_internet_left"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbpsiobegfEx"
      },
      "outputs": [],
      "source": [
        "train1_data = train_internet[common_cols]\n",
        "train2_data = train_bank[common_cols]\n",
        "test_data = test[common_cols[:-1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy7ooStOgfEy"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# 日期类型：issueDate，earliesCreditLine\n",
        "# 转换为pandas中的日期类型\n",
        "train1_data['issue_date'] = pd.to_datetime(train1_data['issue_date'])\n",
        "# 提取多尺度特征\n",
        "train1_data['issue_date_y'] = train1_data['issue_date'].dt.year\n",
        "train1_data['issue_date_m'] = train1_data['issue_date'].dt.month\n",
        "# 提取时间diff\n",
        "# 设置初始的时间\n",
        "base_time = datetime.datetime.strptime('2007-06-01', '%Y-%m-%d')\n",
        "# 转换为天为单位\n",
        "train1_data['issue_date_diff'] = train1_data['issue_date'].apply(lambda x: x-base_time).dt.days\n",
        "train1_data[['issue_date', 'issue_date_y', 'issue_date_m', 'issue_date_diff']]\n",
        "train1_data.drop('issue_date', axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qak8pIgEgfE0"
      },
      "outputs": [],
      "source": [
        "# 日期类型：issueDate，earliesCreditLine\n",
        "# 转换为pandas中的日期类型\n",
        "train2_data['issue_date'] = pd.to_datetime(train2_data['issue_date'])\n",
        "# 提取多尺度特征\n",
        "train2_data['issue_date_y'] = train2_data['issue_date'].dt.year\n",
        "train2_data['issue_date_m'] = train2_data['issue_date'].dt.month\n",
        "# 提取时间diff\n",
        "# 设置初始的时间\n",
        "base_time = datetime.datetime.strptime('2007-06-01', '%Y-%m-%d')\n",
        "# 转换为天为单位\n",
        "train2_data['issue_date_diff'] = train2_data['issue_date'].apply(lambda x: x-base_time).dt.days\n",
        "train2_data[['issue_date', 'issue_date_y', 'issue_date_m', 'issue_date_diff']]\n",
        "train2_data.drop('issue_date', axis = 1, inplace = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWDnOFwGgfE2"
      },
      "outputs": [],
      "source": [
        "employer_type = train1_data['employer_type'].value_counts().index\n",
        "industry = train1_data['industry'].value_counts().index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WX17hwKbgfE3"
      },
      "outputs": [],
      "source": [
        "emp_type_dict = dict(zip(employer_type, [0,1,2,3,4,5]))\n",
        "industry_dict = dict(zip(industry, [i for i in range(15)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOBer9ALgfE4"
      },
      "outputs": [],
      "source": [
        "train1_data['work_year'].dropna()\n",
        "train2_data['work_year'].dropna()\n",
        "\n",
        "work_year_map = {'10+ years': 10, '2 years': 2, '< 1 year': 0, '3 years': 3, '1 year': 1,\n",
        "     '5 years': 5, '4 years': 4, '6 years': 6, '8 years': 8, '7 years': 7, '9 years': 9}\n",
        "train1_data['work_year']  = train1_data['work_year'].map(work_year_map)\n",
        "train2_data['work_year']  = train2_data['work_year'].map(work_year_map)\n",
        "\n",
        "train1_data['class'] = train1_data['class'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6})\n",
        "train2_data['class'] = train2_data['class'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6})\n",
        "\n",
        "train1_data['employer_type'] = train1_data['employer_type'].map(emp_type_dict)\n",
        "train2_data['employer_type'] = train2_data['employer_type'].map(emp_type_dict)\n",
        "\n",
        "train1_data['industry'] = train1_data['industry'].map(industry_dict)\n",
        "train2_data['industry'] = train2_data['industry'].map(industry_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBrlfNGugfE5"
      },
      "outputs": [],
      "source": [
        "# 日期类型：issueDate，earliesCreditLine\n",
        "#train[cat_features]\n",
        "# 转换为pandas中的日期类型\n",
        "test_data['issue_date'] = pd.to_datetime(test_data['issue_date'])\n",
        "# 提取多尺度特征\n",
        "test_data['issue_date_y'] = test_data['issue_date'].dt.year\n",
        "test_data['issue_date_m'] = test_data['issue_date'].dt.month\n",
        "# 提取时间diff\n",
        "# 设置初始的时间\n",
        "base_time = datetime.datetime.strptime('2007-06-01', '%Y-%m-%d')\n",
        "# 转换为天为单位\n",
        "test_data['issue_date_diff'] = test_data['issue_date'].apply(lambda x: x-base_time).dt.days\n",
        "test_data[['issue_date', 'issue_date_y', 'issue_date_m', 'issue_date_diff']]\n",
        "test_data.drop('issue_date', axis = 1, inplace = True)\n",
        "test_data['work_year'].dropna()\n",
        "\n",
        "work_year_map = {'10+ years': 10, '2 years': 2, '< 1 year': 0, '3 years': 3, '1 year': 1,\n",
        "     '5 years': 5, '4 years': 4, '6 years': 6, '8 years': 8, '7 years': 7, '9 years': 9}\n",
        "test_data['work_year']  = test_data['work_year'].map(work_year_map)\n",
        "test_data['class'] = test_data['class'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6})\n",
        "test_data['employer_type'] = test_data['employer_type'].map(emp_type_dict)\n",
        "test_data['industry'] = test_data['industry'].map(industry_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Rd68eBY9U3Z"
      },
      "outputs": [],
      "source": [
        "train1 = train1_data.drop(['loan_id','user_id'], axis = 1, inplace = False)\n",
        "# y_train1 = train_bank[['loan_id','isDefault']]\n",
        "\n",
        "train2 = train2_data.drop(['loan_id','user_id'], axis = 1, inplace = False)\n",
        "# y_train2 = train_internet[['loan_id','isdefault']]\n",
        "# X_train = pd.concat([X_train1, X_train2])\n",
        "# y_train = pd.concat([y_train1, y_train2])\n",
        "total_data = pd.concat([train1,train2]).reset_index(drop=True)\n",
        "total_data = total_data.dropna()\n",
        "\n",
        "# default_df = total_data.loc[total_data['isDefault'] == 1]\n",
        "# ndefault_df = total_data.loc[total_data['isDefault'] == 0][:137888]  ## 555177 not default columns and 137888 default columns\n",
        "# resample_df = pd.concat([default_df, ndefault_df])\n",
        "\n",
        "# Shuffle dataframe rows\n",
        "# new_df = resample_df.sample(frac=1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9v8q0DX0rrE"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clean_mon(x):\n",
        "    mons = {'jan':1, 'feb':2, 'mar':3, 'apr':4,  'may':5,  'jun':6,\n",
        "            'jul':7, 'aug':8, 'sep':9, 'oct':10, 'nov':11, 'dec':12}\n",
        "    year_group = re.search('(\\d+)', x)\n",
        "    if year_group:\n",
        "        year = int(year_group.group(1))\n",
        "        if year < 22:\n",
        "            year += 2000\n",
        "        elif 100 > year > 22:\n",
        "            year += 1900\n",
        "        else:\n",
        "            pass\n",
        "    else:\n",
        "        year = 2022\n",
        "        \n",
        "    month_group = re.search('([a-zA-Z]+)', x)\n",
        "    if month_group:\n",
        "        mon = month_group.group(1).lower()\n",
        "        month = mons[mon]\n",
        "    else:\n",
        "        month = 0\n",
        "        \n",
        "    return year*100 + month\n",
        "\n",
        "total_data['earlies_credit_mon'] = total_data['earlies_credit_mon'].apply(clean_mon)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ht-eQrSiG9fT"
      },
      "outputs": [],
      "source": [
        "test_data['earlies_credit_mon'] = test_data['earlies_credit_mon'].apply(clean_mon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfwZX4DX2raf"
      },
      "outputs": [],
      "source": [
        "X_train = total_data.drop(['isDefault'], axis = 1, inplace = False)\n",
        "y_train = total_data['isDefault'].astype(int)\n",
        "X_test = test_data.drop(['loan_id','user_id'], axis = 1, inplace = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o6_EiUuX0lC"
      },
      "outputs": [],
      "source": [
        "cate_cols = ['class', 'employer_type', 'industry','house_exist','censor_status','region','use','post_code','policy_code','initial_list_status']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rgYITBUgfE6"
      },
      "source": [
        "## 模型使用\n",
        "1) LigthGBM\n",
        "2) NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tClql7jtgfE7"
      },
      "source": [
        "##### 使用internet和bank数据共同特征总量训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "183omGm5p51k"
      },
      "outputs": [],
      "source": [
        "# !pip3 install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLGMeNA3py0D"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "# from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llLEYLlBy-g6",
        "outputId": "461ac1d6-85ed-4f9a-9223-992d1fe10375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "libboost-dev is already the newest version (1.65.1.0ubuntu1).\n",
            "libboost-dev set to manually installed.\n",
            "libboost-filesystem-dev is already the newest version (1.65.1.0ubuntu1).\n",
            "libboost-filesystem-dev set to manually installed.\n",
            "libboost-system-dev is already the newest version (1.65.1.0ubuntu1).\n",
            "libboost-system-dev set to manually installed.\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.9).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  cuda-command-line-tools-10-0 cuda-command-line-tools-10-1\n",
            "  cuda-command-line-tools-11-0 cuda-compiler-10-0 cuda-compiler-10-1\n",
            "  cuda-compiler-11-0 cuda-cuobjdump-10-0 cuda-cuobjdump-10-1\n",
            "  cuda-cuobjdump-11-0 cuda-cupti-10-0 cuda-cupti-10-1 cuda-cupti-11-0\n",
            "  cuda-cupti-dev-11-0 cuda-documentation-10-0 cuda-documentation-10-1\n",
            "  cuda-documentation-11-0 cuda-documentation-11-1 cuda-gdb-10-0 cuda-gdb-10-1\n",
            "  cuda-gdb-11-0 cuda-gpu-library-advisor-10-0 cuda-gpu-library-advisor-10-1\n",
            "  cuda-libraries-10-0 cuda-libraries-10-1 cuda-libraries-11-0\n",
            "  cuda-memcheck-10-0 cuda-memcheck-10-1 cuda-memcheck-11-0 cuda-nsight-10-0\n",
            "  cuda-nsight-10-1 cuda-nsight-11-0 cuda-nsight-11-1 cuda-nsight-compute-10-0\n",
            "  cuda-nsight-compute-10-1 cuda-nsight-compute-11-0 cuda-nsight-compute-11-1\n",
            "  cuda-nsight-systems-10-1 cuda-nsight-systems-11-0 cuda-nsight-systems-11-1\n",
            "  cuda-nvcc-10-0 cuda-nvcc-10-1 cuda-nvcc-11-0 cuda-nvdisasm-10-0\n",
            "  cuda-nvdisasm-10-1 cuda-nvdisasm-11-0 cuda-nvml-dev-10-0 cuda-nvml-dev-10-1\n",
            "  cuda-nvml-dev-11-0 cuda-nvprof-10-0 cuda-nvprof-10-1 cuda-nvprof-11-0\n",
            "  cuda-nvprune-10-0 cuda-nvprune-10-1 cuda-nvprune-11-0 cuda-nvtx-10-0\n",
            "  cuda-nvtx-10-1 cuda-nvtx-11-0 cuda-nvvp-10-0 cuda-nvvp-10-1 cuda-nvvp-11-0\n",
            "  cuda-nvvp-11-1 cuda-samples-10-0 cuda-samples-10-1 cuda-samples-11-0\n",
            "  cuda-samples-11-1 cuda-sanitizer-11-0 cuda-sanitizer-api-10-1\n",
            "  cuda-toolkit-10-0 cuda-toolkit-10-1 cuda-toolkit-11-0 cuda-toolkit-11-1\n",
            "  cuda-tools-10-0 cuda-tools-10-1 cuda-tools-11-0 cuda-tools-11-1\n",
            "  cuda-visual-tools-10-0 cuda-visual-tools-10-1 cuda-visual-tools-11-0\n",
            "  cuda-visual-tools-11-1 default-jre dkms freeglut3 freeglut3-dev\n",
            "  keyboard-configuration libargon2-0 libcap2 libcryptsetup12\n",
            "  libdevmapper1.02.1 libfontenc1 libidn11 libip4tc0 libjansson4\n",
            "  libnvidia-cfg1-510 libnvidia-common-460 libnvidia-common-510\n",
            "  libnvidia-extra-510 libnvidia-fbc1-510 libnvidia-gl-510 libpam-systemd\n",
            "  libpolkit-agent-1-0 libpolkit-backend-1-0 libpolkit-gobject-1-0 libxfont2\n",
            "  libxi-dev libxkbfile1 libxmu-dev libxmu-headers libxnvctrl0 libxtst6\n",
            "  nsight-compute-2020.2.1 nsight-compute-2022.1.0 nsight-systems-2020.3.2\n",
            "  nsight-systems-2020.3.4 nsight-systems-2021.5.2 nvidia-dkms-510\n",
            "  nvidia-kernel-common-510 nvidia-kernel-source-510 nvidia-modprobe\n",
            "  nvidia-settings openjdk-11-jre policykit-1 policykit-1-gnome python3-xkit\n",
            "  screen-resolution-extra systemd systemd-sysv udev x11-xkb-utils\n",
            "  xserver-common xserver-xorg-core-hwe-18.04 xserver-xorg-video-nvidia-510\n",
            "Use 'sudo apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [783 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [21.1 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,470 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,564 kB]\n",
            "Hit:16 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [917 kB]\n",
            "Get:19 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:20 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [29.0 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,248 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,826 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [816 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,004 kB]\n",
            "Get:26 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [936 kB]\n",
            "Get:27 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Get:28 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [42.8 kB]\n",
            "Fetched 15.0 MB in 4s (3,685 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  cuda-command-line-tools-10-0 cuda-command-line-tools-10-1\n",
            "  cuda-command-line-tools-11-0 cuda-compiler-10-0 cuda-compiler-10-1\n",
            "  cuda-compiler-11-0 cuda-cuobjdump-10-0 cuda-cuobjdump-10-1\n",
            "  cuda-cuobjdump-11-0 cuda-cupti-10-0 cuda-cupti-10-1 cuda-cupti-11-0\n",
            "  cuda-cupti-dev-11-0 cuda-documentation-10-0 cuda-documentation-10-1\n",
            "  cuda-documentation-11-0 cuda-documentation-11-1 cuda-gdb-10-0 cuda-gdb-10-1\n",
            "  cuda-gdb-11-0 cuda-gpu-library-advisor-10-0 cuda-gpu-library-advisor-10-1\n",
            "  cuda-libraries-10-0 cuda-libraries-10-1 cuda-libraries-11-0\n",
            "  cuda-memcheck-10-0 cuda-memcheck-10-1 cuda-memcheck-11-0 cuda-nsight-10-0\n",
            "  cuda-nsight-10-1 cuda-nsight-11-0 cuda-nsight-11-1 cuda-nsight-compute-10-0\n",
            "  cuda-nsight-compute-10-1 cuda-nsight-compute-11-0 cuda-nsight-compute-11-1\n",
            "  cuda-nsight-systems-10-1 cuda-nsight-systems-11-0 cuda-nsight-systems-11-1\n",
            "  cuda-nvcc-10-0 cuda-nvcc-10-1 cuda-nvcc-11-0 cuda-nvdisasm-10-0\n",
            "  cuda-nvdisasm-10-1 cuda-nvdisasm-11-0 cuda-nvml-dev-10-0 cuda-nvml-dev-10-1\n",
            "  cuda-nvml-dev-11-0 cuda-nvprof-10-0 cuda-nvprof-10-1 cuda-nvprof-11-0\n",
            "  cuda-nvprune-10-0 cuda-nvprune-10-1 cuda-nvprune-11-0 cuda-nvtx-10-0\n",
            "  cuda-nvtx-10-1 cuda-nvtx-11-0 cuda-nvvp-10-0 cuda-nvvp-10-1 cuda-nvvp-11-0\n",
            "  cuda-nvvp-11-1 cuda-samples-10-0 cuda-samples-10-1 cuda-samples-11-0\n",
            "  cuda-samples-11-1 cuda-sanitizer-11-0 cuda-sanitizer-api-10-1\n",
            "  cuda-toolkit-10-0 cuda-toolkit-10-1 cuda-toolkit-11-0 cuda-toolkit-11-1\n",
            "  cuda-tools-10-0 cuda-tools-10-1 cuda-tools-11-0 cuda-tools-11-1\n",
            "  cuda-visual-tools-10-0 cuda-visual-tools-10-1 cuda-visual-tools-11-0\n",
            "  cuda-visual-tools-11-1 default-jre freeglut3 freeglut3-dev\n",
            "  libnvidia-common-460 libxi-dev libxmu-dev libxmu-headers libxtst6\n",
            "  nsight-compute-2020.2.1 nsight-compute-2022.1.0 nsight-systems-2020.3.2\n",
            "  nsight-systems-2020.3.4 nsight-systems-2021.5.2 nvidia-modprobe\n",
            "  openjdk-11-jre\n",
            "Use 'sudo apt autoremove' to remove them.\n",
            "The following additional packages will be installed:\n",
            "  libnvidia-cfg1-510 libnvidia-common-510 libnvidia-compute-510\n",
            "  libnvidia-decode-510 libnvidia-encode-510 libnvidia-extra-510\n",
            "  libnvidia-fbc1-510 libnvidia-gl-510 nvidia-384 nvidia-compute-utils-510\n",
            "  nvidia-dkms-510 nvidia-driver-418 nvidia-driver-430 nvidia-driver-510\n",
            "  nvidia-kernel-common-510 nvidia-kernel-source-510 nvidia-utils-510\n",
            "  xserver-xorg-video-nvidia-510\n",
            "Recommended packages:\n",
            "  nvidia-prime libnvidia-compute-510:i386 libnvidia-decode-510:i386\n",
            "  libnvidia-encode-510:i386 libnvidia-fbc1-510:i386 libnvidia-gl-510:i386\n",
            "The following packages will be REMOVED:\n",
            "  libnvidia-compute-460\n",
            "The following NEW packages will be installed:\n",
            "  libnvidia-compute-510 libnvidia-decode-510 libnvidia-encode-510 nvidia-375\n",
            "  nvidia-384 nvidia-compute-utils-510 nvidia-driver-418 nvidia-driver-430\n",
            "  nvidia-driver-510 nvidia-utils-510\n",
            "The following packages will be upgraded:\n",
            "  libnvidia-cfg1-510 libnvidia-common-510 libnvidia-extra-510\n",
            "  libnvidia-fbc1-510 libnvidia-gl-510 nvidia-dkms-510 nvidia-kernel-common-510\n",
            "  nvidia-kernel-source-510 xserver-xorg-video-nvidia-510\n",
            "9 upgraded, 10 newly installed, 1 to remove and 56 not upgraded.\n",
            "Need to get 254 MB of archives.\n",
            "After this operation, 45.4 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  xserver-xorg-video-nvidia-510 510.47.03-0ubuntu1 [1,396 kB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libnvidia-cfg1-510 510.47.03-0ubuntu1 [79.3 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libnvidia-extra-510 510.47.03-0ubuntu1 [49.2 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libnvidia-common-510 510.47.03-0ubuntu1 [10.3 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libnvidia-gl-510 510.47.03-0ubuntu1 [168 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 nvidia-driver-418 amd64 430.50-0ubuntu0.18.04.2 [6,836 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 nvidia-375 amd64 384.111-0ubuntu1 [4,880 B]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libnvidia-compute-510 510.47.03-0ubuntu1 [31.4 MB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libnvidia-decode-510 510.47.03-0ubuntu1 [1,252 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libnvidia-encode-510 510.47.03-0ubuntu1 [41.8 kB]\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libnvidia-fbc1-510 510.47.03-0ubuntu1 [46.9 kB]\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nvidia-dkms-510 510.47.03-0ubuntu1 [29.5 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nvidia-kernel-source-510 510.47.03-0ubuntu1 [29.1 MB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nvidia-kernel-common-510 510.47.03-0ubuntu1 [20.5 MB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nvidia-compute-utils-510 510.47.03-0ubuntu1 [351 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nvidia-utils-510 510.47.03-0ubuntu1 [386 kB]\n",
            "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nvidia-driver-510 510.47.03-0ubuntu1 [446 kB]\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nvidia-driver-430 510.47.03-0ubuntu1 [6,884 B]\n",
            "Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nvidia-384 418.226.00-0ubuntu1 [6,848 B]\n",
            "Fetched 254 MB in 10s (26.0 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 19.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 155113 files and directories currently installed.)\n",
            "Removing libnvidia-compute-460:amd64 (460.106.00-0ubuntu1) ...\n",
            "(Reading database ... 155098 files and directories currently installed.)\n",
            "Preparing to unpack .../00-xserver-xorg-video-nvidia-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking xserver-xorg-video-nvidia-510 (510.47.03-0ubuntu1) over (510.39.01-0ubuntu1) ...\n",
            "Preparing to unpack .../01-libnvidia-cfg1-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-cfg1-510:amd64 (510.47.03-0ubuntu1) over (510.39.01-0ubuntu1) ...\n",
            "Preparing to unpack .../02-libnvidia-extra-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-extra-510:amd64 (510.47.03-0ubuntu1) over (510.39.01-0ubuntu1) ...\n",
            "Preparing to unpack .../03-libnvidia-common-510_510.47.03-0ubuntu1_all.deb ...\n",
            "Unpacking libnvidia-common-510 (510.47.03-0ubuntu1) over (510.39.01-0ubuntu1) ...\n",
            "Preparing to unpack .../04-libnvidia-gl-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "dpkg-query: no packages found matching libnvidia-gl-450\n",
            "Unpacking libnvidia-gl-510:amd64 (510.47.03-0ubuntu1) over (510.39.01-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-compute-510:amd64.\n",
            "Preparing to unpack .../05-libnvidia-compute-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-compute-510:amd64 (510.47.03-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-decode-510:amd64.\n",
            "Preparing to unpack .../06-libnvidia-decode-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-decode-510:amd64 (510.47.03-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-encode-510:amd64.\n",
            "Preparing to unpack .../07-libnvidia-encode-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-encode-510:amd64 (510.47.03-0ubuntu1) ...\n",
            "Preparing to unpack .../08-libnvidia-fbc1-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-fbc1-510:amd64 (510.47.03-0ubuntu1) over (510.39.01-0ubuntu1) ...\n",
            "Preparing to unpack .../09-nvidia-dkms-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Removing all DKMS Modules\n",
            "Done.\n",
            "Unpacking nvidia-dkms-510 (510.47.03-0ubuntu1) over (510.39.01-0ubuntu1) ...\n",
            "Preparing to unpack .../10-nvidia-kernel-source-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-kernel-source-510 (510.47.03-0ubuntu1) over (510.39.01-0ubuntu1) ...\n",
            "Preparing to unpack .../11-nvidia-kernel-common-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-kernel-common-510 (510.47.03-0ubuntu1) over (510.39.01-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-compute-utils-510.\n",
            "Preparing to unpack .../12-nvidia-compute-utils-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-compute-utils-510 (510.47.03-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-utils-510.\n",
            "Preparing to unpack .../13-nvidia-utils-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-utils-510 (510.47.03-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-driver-510.\n",
            "Preparing to unpack .../14-nvidia-driver-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-driver-510 (510.47.03-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-driver-430.\n",
            "Preparing to unpack .../15-nvidia-driver-430_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-driver-430 (510.47.03-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-driver-418.\n",
            "Preparing to unpack .../16-nvidia-driver-418_430.50-0ubuntu0.18.04.2_amd64.deb ...\n",
            "Unpacking nvidia-driver-418 (430.50-0ubuntu0.18.04.2) ...\n",
            "Selecting previously unselected package nvidia-384.\n",
            "Preparing to unpack .../17-nvidia-384_418.226.00-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-384 (418.226.00-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-375.\n",
            "Preparing to unpack .../18-nvidia-375_384.111-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-375 (384.111-0ubuntu1) ...\n",
            "Setting up nvidia-kernel-source-510 (510.47.03-0ubuntu1) ...\n",
            "Setting up libnvidia-compute-510:amd64 (510.47.03-0ubuntu1) ...\n",
            "Setting up libnvidia-decode-510:amd64 (510.47.03-0ubuntu1) ...\n",
            "Setting up libnvidia-fbc1-510:amd64 (510.47.03-0ubuntu1) ...\n",
            "Setting up libnvidia-cfg1-510:amd64 (510.47.03-0ubuntu1) ...\n",
            "Setting up nvidia-utils-510 (510.47.03-0ubuntu1) ...\n",
            "Setting up nvidia-kernel-common-510 (510.47.03-0ubuntu1) ...\n",
            "Setting up xserver-xorg-video-nvidia-510 (510.47.03-0ubuntu1) ...\n",
            "Setting up libnvidia-extra-510:amd64 (510.47.03-0ubuntu1) ...\n",
            "Setting up libnvidia-common-510 (510.47.03-0ubuntu1) ...\n",
            "Setting up nvidia-compute-utils-510 (510.47.03-0ubuntu1) ...\n",
            "Warning: The home dir /nonexistent you specified can't be accessed: No such file or directory\n",
            "Adding system user `nvidia-persistenced' (UID 104) ...\n",
            "Adding new group `nvidia-persistenced' (GID 108) ...\n",
            "Adding new user `nvidia-persistenced' (UID 104) with group `nvidia-persistenced' ...\n",
            "Not creating home directory `/nonexistent'.\n",
            "Setting up libnvidia-encode-510:amd64 (510.47.03-0ubuntu1) ...\n",
            "Setting up nvidia-dkms-510 (510.47.03-0ubuntu1) ...\n",
            "\n",
            "A modprobe blacklist file has been created at /etc/modprobe.d to prevent Nouveau\n",
            "from loading. This can be reverted by deleting the following file:\n",
            "/etc/modprobe.d/nvidia-graphics-drivers.conf\n",
            "\n",
            "A new initrd image has also been created. To revert, please regenerate your\n",
            "initrd by running the following command after deleting the modprobe.d file:\n",
            "`/usr/sbin/initramfs -u`\n",
            "\n",
            "*****************************************************************************\n",
            "*** Reboot your computer and verify that the NVIDIA graphics driver can   ***\n",
            "*** be loaded.                                                            ***\n",
            "*****************************************************************************\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Loading new nvidia-510.47.03 DKMS files...\n",
            "It is likely that 5.4.144+ belongs to a chroot's host\n",
            "Building for 4.15.0-167-generic\n",
            "Building for architecture x86_64\n",
            "Building initial module for 4.15.0-167-generic\n",
            "Done.\n",
            "\n",
            "nvidia:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/4.15.0-167-generic/updates/dkms/\n",
            "\n",
            "nvidia-modeset.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/4.15.0-167-generic/updates/dkms/\n",
            "\n",
            "nvidia-drm.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/4.15.0-167-generic/updates/dkms/\n",
            "\n",
            "nvidia-peermem.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/4.15.0-167-generic/updates/dkms/\n",
            "\n",
            "nvidia-uvm.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/4.15.0-167-generic/updates/dkms/\n",
            "\n",
            "depmod...\n",
            "\n",
            "DKMS: install completed.\n",
            "Setting up libnvidia-gl-510:amd64 (510.47.03-0ubuntu1) ...\n",
            "Setting up nvidia-driver-510 (510.47.03-0ubuntu1) ...\n",
            "Setting up nvidia-driver-430 (510.47.03-0ubuntu1) ...\n",
            "Setting up nvidia-driver-418 (430.50-0ubuntu0.18.04.2) ...\n",
            "Setting up nvidia-384 (418.226.00-0ubuntu1) ...\n",
            "Setting up nvidia-375 (384.111-0ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dbus (1.12.2-1ubuntu1.2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "nvidia-opencl-dev is already the newest version (9.1.85-3ubuntu1).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  cuda-command-line-tools-10-0 cuda-command-line-tools-10-1\n",
            "  cuda-command-line-tools-11-0 cuda-compiler-10-0 cuda-compiler-10-1\n",
            "  cuda-compiler-11-0 cuda-cuobjdump-10-0 cuda-cuobjdump-10-1\n",
            "  cuda-cuobjdump-11-0 cuda-cupti-10-0 cuda-cupti-10-1 cuda-cupti-11-0\n",
            "  cuda-cupti-dev-11-0 cuda-documentation-10-0 cuda-documentation-10-1\n",
            "  cuda-documentation-11-0 cuda-documentation-11-1 cuda-gdb-10-0 cuda-gdb-10-1\n",
            "  cuda-gdb-11-0 cuda-gpu-library-advisor-10-0 cuda-gpu-library-advisor-10-1\n",
            "  cuda-libraries-10-0 cuda-libraries-10-1 cuda-libraries-11-0\n",
            "  cuda-memcheck-10-0 cuda-memcheck-10-1 cuda-memcheck-11-0 cuda-nsight-10-0\n",
            "  cuda-nsight-10-1 cuda-nsight-11-0 cuda-nsight-11-1 cuda-nsight-compute-10-0\n",
            "  cuda-nsight-compute-10-1 cuda-nsight-compute-11-0 cuda-nsight-compute-11-1\n",
            "  cuda-nsight-systems-10-1 cuda-nsight-systems-11-0 cuda-nsight-systems-11-1\n",
            "  cuda-nvcc-10-0 cuda-nvcc-10-1 cuda-nvcc-11-0 cuda-nvdisasm-10-0\n",
            "  cuda-nvdisasm-10-1 cuda-nvdisasm-11-0 cuda-nvml-dev-10-0 cuda-nvml-dev-10-1\n",
            "  cuda-nvml-dev-11-0 cuda-nvprof-10-0 cuda-nvprof-10-1 cuda-nvprof-11-0\n",
            "  cuda-nvprune-10-0 cuda-nvprune-10-1 cuda-nvprune-11-0 cuda-nvtx-10-0\n",
            "  cuda-nvtx-10-1 cuda-nvtx-11-0 cuda-nvvp-10-0 cuda-nvvp-10-1 cuda-nvvp-11-0\n",
            "  cuda-nvvp-11-1 cuda-samples-10-0 cuda-samples-10-1 cuda-samples-11-0\n",
            "  cuda-samples-11-1 cuda-sanitizer-11-0 cuda-sanitizer-api-10-1\n",
            "  cuda-toolkit-10-0 cuda-toolkit-10-1 cuda-toolkit-11-0 cuda-toolkit-11-1\n",
            "  cuda-tools-10-0 cuda-tools-10-1 cuda-tools-11-0 cuda-tools-11-1\n",
            "  cuda-visual-tools-10-0 cuda-visual-tools-10-1 cuda-visual-tools-11-0\n",
            "  cuda-visual-tools-11-1 default-jre freeglut3 freeglut3-dev\n",
            "  libnvidia-common-460 libxi-dev libxmu-dev libxmu-headers libxtst6\n",
            "  nsight-compute-2020.2.1 nsight-compute-2022.1.0 nsight-systems-2020.3.2\n",
            "  nsight-systems-2020.3.4 nsight-systems-2021.5.2 nvidia-modprobe\n",
            "  openjdk-11-jre\n",
            "Use 'sudo apt autoremove' to remove them.\n",
            "The following additional packages will be installed:\n",
            "  nvidia-headless-418 nvidia-headless-430 nvidia-headless-510\n",
            "  nvidia-headless-no-dkms-510 nvidia-opencl-icd-384 opencl-clhpp-headers\n",
            "Suggested packages:\n",
            "  opencl-clhpp-headers-doc\n",
            "The following NEW packages will be installed:\n",
            "  nvidia-headless-418 nvidia-headless-430 nvidia-headless-510\n",
            "  nvidia-headless-no-dkms-510 nvidia-opencl-icd-375 nvidia-opencl-icd-384\n",
            "  opencl-clhpp-headers opencl-headers\n",
            "0 upgraded, 8 newly installed, 0 to remove and 56 not upgraded.\n",
            "Need to get 101 kB of archives.\n",
            "After this operation, 751 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 nvidia-headless-418 amd64 430.50-0ubuntu0.18.04.2 [6,852 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nvidia-headless-no-dkms-510 510.47.03-0ubuntu1 [7,112 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 nvidia-opencl-icd-375 amd64 384.111-0ubuntu1 [4,896 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 opencl-clhpp-headers all 2.0.10+git12-g5dd8bb9-1 [58.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 opencl-headers all 2.2~2018.02.21-gb5c3680-1 [2,728 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nvidia-headless-510 510.47.03-0ubuntu1 [6,988 B]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nvidia-headless-430 510.47.03-0ubuntu1 [6,888 B]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nvidia-opencl-icd-384 418.226.00-0ubuntu1 [6,872 B]\n",
            "Fetched 101 kB in 1s (169 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 8.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package nvidia-headless-no-dkms-510.\n",
            "(Reading database ... 155237 files and directories currently installed.)\n",
            "Preparing to unpack .../0-nvidia-headless-no-dkms-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-headless-no-dkms-510 (510.47.03-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-headless-510.\n",
            "Preparing to unpack .../1-nvidia-headless-510_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-headless-510 (510.47.03-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-headless-430:amd64.\n",
            "Preparing to unpack .../2-nvidia-headless-430_510.47.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-headless-430:amd64 (510.47.03-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-headless-418:amd64.\n",
            "Preparing to unpack .../3-nvidia-headless-418_430.50-0ubuntu0.18.04.2_amd64.deb ...\n",
            "Unpacking nvidia-headless-418:amd64 (430.50-0ubuntu0.18.04.2) ...\n",
            "Selecting previously unselected package nvidia-opencl-icd-384.\n",
            "Preparing to unpack .../4-nvidia-opencl-icd-384_418.226.00-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-opencl-icd-384 (418.226.00-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-opencl-icd-375.\n",
            "Preparing to unpack .../5-nvidia-opencl-icd-375_384.111-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-opencl-icd-375 (384.111-0ubuntu1) ...\n",
            "Selecting previously unselected package opencl-clhpp-headers.\n",
            "Preparing to unpack .../6-opencl-clhpp-headers_2.0.10+git12-g5dd8bb9-1_all.deb ...\n",
            "Unpacking opencl-clhpp-headers (2.0.10+git12-g5dd8bb9-1) ...\n",
            "Selecting previously unselected package opencl-headers.\n",
            "Preparing to unpack .../7-opencl-headers_2.2~2018.02.21-gb5c3680-1_all.deb ...\n",
            "Unpacking opencl-headers (2.2~2018.02.21-gb5c3680-1) ...\n",
            "Setting up nvidia-headless-no-dkms-510 (510.47.03-0ubuntu1) ...\n",
            "Setting up opencl-clhpp-headers (2.0.10+git12-g5dd8bb9-1) ...\n",
            "Setting up nvidia-headless-510 (510.47.03-0ubuntu1) ...\n",
            "Setting up opencl-headers (2.2~2018.02.21-gb5c3680-1) ...\n",
            "Setting up nvidia-headless-430:amd64 (510.47.03-0ubuntu1) ...\n",
            "Setting up nvidia-headless-418:amd64 (430.50-0ubuntu0.18.04.2) ...\n",
            "Setting up nvidia-opencl-icd-384 (418.226.00-0ubuntu1) ...\n",
            "Setting up nvidia-opencl-icd-375 (384.111-0ubuntu1) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install --no-install-recommends git cmake build-essential libboost-dev libboost-system-dev libboost-filesystem-dev\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install --no-install-recommends nvidia-375\n",
        "!sudo apt-get install --no-install-recommends nvidia-opencl-icd-375 nvidia-opencl-dev opencl-headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rdG3tkRlQkU",
        "outputId": "295479a7-9623-4191-b1d0-4a7a754d58dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LightGBM'...\n",
            "remote: Enumerating objects: 25098, done.\u001b[K\n",
            "remote: Counting objects: 100% (774/774), done.\u001b[K\n",
            "remote: Compressing objects: 100% (502/502), done.\u001b[K\n",
            "remote: Total 25098 (delta 488), reused 446 (delta 268), pack-reused 24324\u001b[K\n",
            "Receiving objects: 100% (25098/25098), 18.79 MiB | 22.32 MiB/s, done.\n",
            "Resolving deltas: 100% (18434/18434), done.\n",
            "Submodule 'include/boost/compute' (https://github.com/boostorg/compute) registered for path 'external_libs/compute'\n",
            "Submodule 'eigen' (https://gitlab.com/libeigen/eigen.git) registered for path 'external_libs/eigen'\n",
            "Submodule 'external_libs/fast_double_parser' (https://github.com/lemire/fast_double_parser.git) registered for path 'external_libs/fast_double_parser'\n",
            "Submodule 'external_libs/fmt' (https://github.com/fmtlib/fmt.git) registered for path 'external_libs/fmt'\n",
            "Cloning into '/content/LightGBM/external_libs/compute'...\n",
            "remote: Enumerating objects: 21733, done.        \n",
            "remote: Counting objects: 100% (5/5), done.        \n",
            "remote: Compressing objects: 100% (5/5), done.        \n",
            "remote: Total 21733 (delta 1), reused 2 (delta 0), pack-reused 21728        \n",
            "Receiving objects: 100% (21733/21733), 8.51 MiB | 21.21 MiB/s, done.\n",
            "Resolving deltas: 100% (17567/17567), done.\n",
            "Cloning into '/content/LightGBM/external_libs/eigen'...\n",
            "remote: Enumerating objects: 114731, done.        \n",
            "remote: Counting objects: 100% (1942/1942), done.        \n",
            "remote: Compressing objects: 100% (901/901), done.        \n",
            "remote: Total 114731 (delta 1287), reused 1640 (delta 1039), pack-reused 112789        \n",
            "Receiving objects: 100% (114731/114731), 103.34 MiB | 23.42 MiB/s, done.\n",
            "Resolving deltas: 100% (94144/94144), done.\n",
            "Cloning into '/content/LightGBM/external_libs/fast_double_parser'...\n",
            "remote: Enumerating objects: 692, done.        \n",
            "remote: Counting objects: 100% (192/192), done.        \n",
            "remote: Compressing objects: 100% (124/124), done.        \n",
            "remote: Total 692 (delta 95), reused 99 (delta 41), pack-reused 500        \n",
            "Receiving objects: 100% (692/692), 802.86 KiB | 6.86 MiB/s, done.\n",
            "Resolving deltas: 100% (349/349), done.\n",
            "Cloning into '/content/LightGBM/external_libs/fmt'...\n",
            "remote: Enumerating objects: 28849, done.        \n",
            "remote: Counting objects: 100% (209/209), done.        \n",
            "remote: Compressing objects: 100% (72/72), done.        \n",
            "remote: Total 28849 (delta 120), reused 178 (delta 104), pack-reused 28640        \n",
            "Receiving objects: 100% (28849/28849), 13.78 MiB | 20.88 MiB/s, done.\n",
            "Resolving deltas: 100% (19474/19474), done.\n",
            "Submodule path 'external_libs/compute': checked out '36350b7de849300bd3d72a05d8bf890ca405a014'\n",
            "Submodule path 'external_libs/eigen': checked out '3147391d946bb4b6c68edd901f2add6ac1f31f8c'\n",
            "Submodule path 'external_libs/fast_double_parser': checked out 'ace60646c02dc54c57f19d644e49a61e7e7758ec'\n",
            "Submodule 'benchmark/dependencies/abseil-cpp' (https://github.com/abseil/abseil-cpp.git) registered for path 'external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp'\n",
            "Submodule 'benchmark/dependencies/double-conversion' (https://github.com/google/double-conversion.git) registered for path 'external_libs/fast_double_parser/benchmarks/dependencies/double-conversion'\n",
            "Cloning into '/content/LightGBM/external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp'...\n",
            "remote: Enumerating objects: 16234, done.        \n",
            "remote: Counting objects: 100% (360/360), done.        \n",
            "remote: Compressing objects: 100% (265/265), done.        \n",
            "remote: Total 16234 (delta 197), reused 175 (delta 95), pack-reused 15874        \n",
            "Receiving objects: 100% (16234/16234), 10.55 MiB | 18.47 MiB/s, done.\n",
            "Resolving deltas: 100% (12479/12479), done.\n",
            "Cloning into '/content/LightGBM/external_libs/fast_double_parser/benchmarks/dependencies/double-conversion'...\n",
            "remote: Enumerating objects: 1338, done.        \n",
            "remote: Counting objects: 100% (182/182), done.        \n",
            "remote: Compressing objects: 100% (140/140), done.        \n",
            "remote: Total 1338 (delta 98), reused 85 (delta 35), pack-reused 1156        \n",
            "Receiving objects: 100% (1338/1338), 7.14 MiB | 17.09 MiB/s, done.\n",
            "Resolving deltas: 100% (870/870), done.\n",
            "Submodule path 'external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp': checked out 'd936052d32a5b7ca08b0199a6724724aea432309'\n",
            "Submodule path 'external_libs/fast_double_parser/benchmarks/dependencies/double-conversion': checked out 'f4cb2384efa55dee0e6652f8674b05763441ab09'\n",
            "Submodule path 'external_libs/fmt': checked out 'b6f4ceaed0a0a24ccf575fab6c56dd50ccf6f1a9'\n"
          ]
        }
      ],
      "source": [
        "!git clone --recursive https://github.com/Microsoft/LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0u6ZxDSy5N_Y",
        "outputId": "795d1fe0-9fca-452b-ba6f-483511fae930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LightGBM\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
            "-- Performing Test MM_PREFETCH\n",
            "-- Performing Test MM_PREFETCH - Success\n",
            "-- Using _mm_prefetch\n",
            "-- Performing Test MM_MALLOC\n",
            "-- Performing Test MM_MALLOC - Success\n",
            "-- Using _mm_malloc\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "CMake Warning:\n",
            "  Manually-specified variables were not used by the project:\n",
            "\n",
            "    -DUSE_GPU\n",
            "\n",
            "\n",
            "-- Build files have been written to: /content/LightGBM\n",
            "\u001b[35m\u001b[1mScanning dependencies of target lightgbm_capi_objs\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target lightgbm_objs\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_capi_objs.dir/src/c_api.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/boosting.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/gbdt.cpp.o\u001b[0m\n",
            "[  8%] Built target lightgbm_capi_objs\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/gbdt_model_text.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/gbdt_prediction.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/prediction_early_stop.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/bin.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/config.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/config_auto.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/dataset.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/dataset_loader.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/file_io.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/json11.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/metadata.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/parser.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/train_share_states.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/tree.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/metric/dcg_calculator.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/metric/metric.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/ifaddrs_patch.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/linker_topo.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/linkers_mpi.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/linkers_socket.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/network.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/objective/objective_function.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda_tree_learner.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/data_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/feature_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/gpu_tree_learner.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/linear_tree_learner.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/serial_tree_learner.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/tree_learner.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/voting_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 89%] Built target lightgbm_objs\n",
            "\u001b[35m\u001b[1mScanning dependencies of target _lightgbm\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target lightgbm\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX shared library lib_lightgbm.so\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/main.cpp.o\u001b[0m\n",
            "[ 94%] Built target _lightgbm\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/application/application.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable lightgbm\u001b[0m\n",
            "[100%] Built target lightgbm\n"
          ]
        }
      ],
      "source": [
        "%cd /content/LightGBM\n",
        "!mkdir build\n",
        "!cmake -D -DUSE_GPU=1 #avoid ..\n",
        "!make -j$(nproc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Uc5LGP4MDt3I"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/guolinke/boosting_tree_benchmarks.git\n",
        "# !cd boosting_tree_benchmarks/data\n",
        "# !wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\"\n",
        "# !gunzip HIGGS.csv.gz\n",
        "# !ls -ltra boosting_tree_benchmarks/data\n",
        "# !python boosting_tree_benchmarks/data/higgs2libsvm.py\n",
        "# !cd ../..\n",
        "# !ln -s boosting_tree_benchmarks/data/higgs.train\n",
        "# !ln -s boosting_tree_benchmarks/data/higgs.test\n",
        "with open('lightgbm_gpu.conf', 'w') as f:\n",
        "  f.write('''max_bin = 63\n",
        "num_leaves = 255\n",
        "num_iterations = 50\n",
        "learning_rate = 0.1\n",
        "tree_learner = serial\n",
        "task = train\n",
        "is_training_metric = false\n",
        "min_data_in_leaf = 1\n",
        "min_sum_hessian_in_leaf = 100\n",
        "ndcg_eval_at = 1,3,5,10\n",
        "device = gpu\n",
        "num_threads=2\n",
        "''')\n",
        "# !./lightgbm config=lightgbm_gpu.conf data=higgs.train valid=higgs.test objective=binary metric=auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT4YzWkElThx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S4a_EmSl5F86"
      },
      "outputs": [],
      "source": [
        "# !rm -rf \n",
        "\n",
        "# !./lightgbm config=lightgbm_gpu.conf data=higgs.train valid=higgs.test objective=binary metric=auc/\n",
        "# !./LightGBM/lightgbm config=LightGBM/lightgbm_gpu.conf data=LightGBM/higgs.train valid=LightGBM/higgs.test objective=binary metric=auc\n",
        "# !./lightgbm config=lightgbm_gpu.conf data=higgs.train objective=binary metric=auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "scWUziJoJMDK"
      },
      "outputs": [],
      "source": [
        "# !pwd\n",
        "# !cd LightGBM/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "leBE_r3e5R_q",
        "outputId": "08d0cb0d-474a-48ad-888a-269db1bc7c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 <command> [options]\n",
            "\n",
            "no such option: --upgrade\n"
          ]
        }
      ],
      "source": [
        "!pip --upgrade --force-reinstall install lightgbm --install-option=--gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e7mvXhtb5uXx",
        "outputId": "d161edf9-d4e1-4311-aeb2-ad7f4fc77e9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LightGBM/python-package\n",
            "total 48\n",
            "-rw-r--r--  1 root root 14454 Feb 18 08:58 README.rst\n",
            "-rw-r--r--  1 root root  2369 Feb 18 08:58 MANIFEST.in\n",
            "drwxr-xr-x  2 root root  4096 Feb 18 08:58 lightgbm\n",
            "-rw-r--r--  1 root root 15696 Feb 18 08:58 setup.py\n",
            "drwxr-xr-x  3 root root  4096 Feb 18 08:58 .\n",
            "drwxr-xr-x 22 root root  4096 Feb 18 09:01 ..\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "running install\n",
            "running build\n",
            "running build_py\n",
            "INFO:root:Generating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO:root:Generating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/lightgbm\n",
            "copying lightgbm/sklearn.py -> build/lib/lightgbm\n",
            "copying lightgbm/callback.py -> build/lib/lightgbm\n",
            "copying lightgbm/libpath.py -> build/lib/lightgbm\n",
            "copying lightgbm/__init__.py -> build/lib/lightgbm\n",
            "copying lightgbm/dask.py -> build/lib/lightgbm\n",
            "copying lightgbm/plotting.py -> build/lib/lightgbm\n",
            "copying lightgbm/compat.py -> build/lib/lightgbm\n",
            "copying lightgbm/basic.py -> build/lib/lightgbm\n",
            "copying lightgbm/engine.py -> build/lib/lightgbm\n",
            "running egg_info\n",
            "creating lightgbm.egg-info\n",
            "writing lightgbm.egg-info/PKG-INFO\n",
            "writing dependency_links to lightgbm.egg-info/dependency_links.txt\n",
            "writing requirements to lightgbm.egg-info/requires.txt\n",
            "writing top-level names to lightgbm.egg-info/top_level.txt\n",
            "writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "no previously-included directories found matching 'build'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching '*.txt'\n",
            "warning: no files found matching '*.so' under directory 'lightgbm'\n",
            "warning: no files found matching 'compile/CMakeLists.txt'\n",
            "warning: no files found matching 'compile/cmake/IntegratedOpenCL.cmake'\n",
            "warning: no files found matching '*.so' under directory 'compile'\n",
            "warning: no files found matching '*.dll' under directory 'compile/Release'\n",
            "warning: no files found matching 'compile/external_libs/compute/CMakeLists.txt'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/compute/cmake'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/compute/include'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/compute/meta'\n",
            "warning: no files found matching 'compile/external_libs/eigen/CMakeLists.txt'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Cholesky'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Core'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Dense'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Eigenvalues'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Geometry'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Householder'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Jacobi'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/LU'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/QR'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/SVD'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Cholesky'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Core'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Eigenvalues'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Geometry'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Householder'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Jacobi'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/LU'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/misc'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/plugins'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/QR'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/SVD'\n",
            "warning: no files found matching 'compile/external_libs/fast_double_parser/CMakeLists.txt'\n",
            "warning: no files found matching 'compile/external_libs/fast_double_parser/LICENSE'\n",
            "warning: no files found matching 'compile/external_libs/fast_double_parser/LICENSE.BSL'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/fast_double_parser/include'\n",
            "warning: no files found matching 'compile/external_libs/fmt/CMakeLists.txt'\n",
            "warning: no files found matching 'compile/external_libs/fmt/LICENSE.rst'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/fmt/include'\n",
            "warning: no files found matching '*' under directory 'compile/include'\n",
            "warning: no files found matching '*' under directory 'compile/src'\n",
            "warning: no files found matching 'LightGBM.sln' under directory 'compile/windows'\n",
            "warning: no files found matching 'LightGBM.vcxproj' under directory 'compile/windows'\n",
            "warning: no files found matching '*.dll' under directory 'compile/windows/x64/DLL'\n",
            "warning: no previously-included files matching '*.py[co]' found anywhere in distribution\n",
            "warning: no previously-included files found matching 'compile/external_libs/compute/.git'\n",
            "writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "copying lightgbm/VERSION.txt -> build/lib/lightgbm\n",
            "running install_lib\n",
            "copying build/lib/lightgbm/sklearn.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/callback.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/libpath.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/__init__.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/dask.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/plotting.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/compat.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/basic.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/engine.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/VERSION.txt -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "INFO:LightGBM:Installing lib_lightgbm from: ['/content/LightGBM/lib_lightgbm.so']\n",
            "copying /content/LightGBM/lib_lightgbm.so -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/sklearn.py to sklearn.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/callback.py to callback.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/libpath.py to libpath.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/dask.py to dask.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/plotting.py to plotting.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/compat.py to compat.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/basic.py to basic.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/engine.py to engine.cpython-37.pyc\n",
            "running install_egg_info\n",
            "Copying lightgbm.egg-info to /usr/local/lib/python3.7/dist-packages/lightgbm-3.3.2.99-py3.7.egg-info\n",
            "running install_scripts\n"
          ]
        }
      ],
      "source": [
        "%cd /content/LightGBM/python-package\n",
        "!ls -ltra\n",
        "!pip install setuptools\n",
        "!sudo python setup.py install --precompile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lecQ50C6q49-"
      },
      "outputs": [],
      "source": [
        "lgb_params = {\n",
        "        'objective': 'binary',\n",
        "        'boosting': 'gbdt',\n",
        "        'n_estimators': 20000,\n",
        "        'random_state': 42,\n",
        "        'learning_rate': 8e-3,\n",
        "        'subsample': 0.6,\n",
        "        'subsample_freq': 1,\n",
        "        'colsample_bytree': 0.4,\n",
        "        # 'reg_alpha': 10.0,\n",
        "        # 'reg_lambda': 1e-1,\n",
        "        'min_child_weight': 256,\n",
        "        'min_child_samples': 500,\n",
        "        'device' : 'gpu',\n",
        "        'lambda_l1': 5, # L1 regularization\n",
        "        'lambda_l2': 10 # L2 regularization\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "xgb_params = {'n_estimators': 10000,\n",
        "        'learning_rate': 0.03689407512484644,\n",
        "        'max_depth': 8,\n",
        "        'colsample_bytree': 0.3723914688159835,\n",
        "        'subsample': 0.780714581166012,\n",
        "        'eval_metric': 'auc',\n",
        "        'use_label_encoder': False,\n",
        "        'gamma': 0,\n",
        "        # 'reg_lambda': 50.0,\n",
        "        'random_state': 42,\n",
        "        'device' : 'gpu',\n",
        "        'lambda_l1': 5, # L1 regularization\n",
        "        'lambda_l2': 10 # L2 regularization\n",
        "}\n",
        "\n",
        "# cat_params = {'iterations': 17298,\n",
        "#         'learning_rate': 0.03429054860458741,\n",
        "#         'reg_lambda': 0.3242286463210283,\n",
        "#         'subsample': 0.9433911589913944,\n",
        "#         'random_strength': 22.4849972385133,\n",
        "#         'depth': 8,\n",
        "#         'min_data_in_leaf': 4,\n",
        "#         'leaf_estimation_iterations': 8,\n",
        "#         'task_type':\"GPU\",\n",
        "#         'bootstrap_type':'Poisson',\n",
        "#         'verbose' : 500,\n",
        "#         'early_stopping_rounds' : 200,\n",
        "#         'eval_metric' : 'AUC'}\n",
        "lgb = LGBMClassifier(**lgb_params)\n",
        "xgb = XGBClassifier(**xgb_params)\n",
        "# cat = CatBoostClassifier(**cat_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z5LK8-rPq9Ef"
      },
      "outputs": [],
      "source": [
        "def get_oof(feats, target, test, kfold, clf):\n",
        "  oof_preds = np.zeros(feats.shape[0])\n",
        "  sub_preds = np.zeros(test.shape[0]) \n",
        "  for fold_, (train_idx, valid_idx) in enumerate(kfold.split(feats,target)):\n",
        "    print(\"fold n°{}\".format(fold_ + 1))\n",
        "    train_X, train_y = feats.iloc[train_idx], target.iloc[train_idx]\n",
        "    valid_X, valid_y = feats.iloc[valid_idx], target.iloc[valid_idx]\n",
        "\n",
        "    clf.fit(train_X, train_y, eval_set = [(valid_X, valid_y)], verbose = 500, early_stopping_rounds = 500, )\n",
        "    oof_preds[valid_idx] = clf.predict_proba(valid_X)[:,1]\n",
        "    sub_preds += clf.predict_proba(test)[:,1]\n",
        "    del train_X, train_y, valid_X, valid_y\n",
        "    gc.collect()\n",
        "\n",
        "  evalution_result = roc_auc_score(target, oof_preds)\n",
        "  print('*'*10)\n",
        "  print('roc auc score:', evalution_result)\n",
        "  print('*'*20)\n",
        "  sub_preds_result = sub_preds / kfold.n_splits\n",
        "  return oof_preds ,sub_preds_result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wwx1UBQPsL2v",
        "outputId": "84570bb5-b467-4824-9cbd-6a9b9bace4bd"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f5abb875b0d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2019\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moof_preds_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_preds_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moof_preds_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_preds_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# oof_preds_3, sub_preds_3 = get_oof(X_train, y_train, X_test, kfold, cat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'StratifiedKFold' is not defined"
          ]
        }
      ],
      "source": [
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
        "oof_preds_1, sub_preds_1 = get_oof(X_train, y_train, X_test, kfold, lgb)\n",
        "oof_preds_2, sub_preds_2 = get_oof(X_train, y_train, X_test, kfold, xgb)\n",
        "# oof_preds_3, sub_preds_3 = get_oof(X_train, y_train, X_test, kfold, cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gkXtz4UsuYaO"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "import numpy as np\n",
        "\n",
        "def stack_model(train_stack, test_stack, y):  #oof_set =[oof_1, oof_2, oof_3, ..., oof_n], predictions_set =[predictions_1, predictions_2, predictions_3, ..., predictions_n],\n",
        "\n",
        "    oof = np.zeros((train_stack.shape[0],))\n",
        "    predictions = np.zeros((test_stack.shape[0],))\n",
        "    scores = []\n",
        "\n",
        "    for fold_, (trn_idx, val_idx) in enumerate(kfold.split(train_stack, y)):\n",
        "        trn_data, trn_y = train_stack.iloc[trn_idx], y.iloc[trn_idx]\n",
        "        val_data, val_y = train_stack.iloc[val_idx], y.iloc[val_idx]\n",
        "\n",
        "        clf = RidgeClassifier(random_state=2099)\n",
        "        clf.fit(trn_data, trn_y)\n",
        "\n",
        "        oof[val_idx] = clf._predict_proba_lr(val_data)[:,1]\n",
        "        predictions +=clf._predict_proba_lr(test_stack)[:,1] / kfold.n_splits\n",
        "\n",
        "        score_single = roc_auc_score(val_y, oof[val_idx])\n",
        "        scores.append(score_single)\n",
        "    print('mean: ', np.mean(scores))\n",
        "\n",
        "    return oof, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_O_uR_mludKm"
      },
      "outputs": [],
      "source": [
        "pred_matrix = np.hstack([sub_preds_1[:,np.newaxis], sub_preds_2[:,np.newaxis]])\n",
        "oof_matrix = np.hstack([oof_preds_1[:,np.newaxis], oof_preds_2[:,np.newaxis]])\n",
        "pred_df = pd.DataFrame(pred_matrix)\n",
        "oof_df = pd.DataFrame(oof_matrix)\n",
        "oof_stack, predictions_stack = stack_model(oof_df, pred_df, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5tDv9tFjKC7s"
      },
      "outputs": [],
      "source": [
        "# submission\n",
        "submission = pd.DataFrame({'id':test_data['loan_id'], 'isDefault':predictions_stack})\n",
        "submission.to_csv('submission_new.csv', index = None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EryR391KV8DK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hyfcUo5hXu6M"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "from sklearn.model_selection import (train_test_split, GridSearchCV)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import (roc_curve, auc, accuracy_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6Xrrh_nngfE8"
      },
      "outputs": [],
      "source": [
        "import lightgbm\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "F80RayLAZKQ4"
      },
      "outputs": [],
      "source": [
        "#Initiate a model\n",
        "params = {\n",
        "    'application': 'binary', # for binary classification\n",
        "#     'num_class' : 1, # used for multi-classes\n",
        "    'boosting': 'dart', # traditional gradient boosting decision tree\n",
        "    'num_iterations': 500, \n",
        "    'learning_rate': 0.005,\n",
        "    'num_leaves': 50,\n",
        "    'device': 'cpu', # you can use GPU to achieve faster learning\n",
        "    'max_depth': -1, # <0 means no limit\n",
        "    'max_bin': 400, # Small number of bins may reduce training accuracy but can deal with over-fitting\n",
        "    'lambda_l1': 5, # L1 regularization\n",
        "    'lambda_l2': 10, # L2 regularization\n",
        "    'metric' : 'binary_error',\n",
        "    'subsample_for_bin': 200, # number of samples for constructing bins\n",
        "    'subsample': 1, # subsample ratio of the training instance\n",
        "    'colsample_bytree': 0.8, # subsample ratio of columns when constructing the tree\n",
        "    'min_split_gain': 0.5, # minimum loss reduction required to make further partition on a leaf node of the tree\n",
        "    'min_child_weight': 1, # minimum sum of instance weight (hessian) needed in a leaf\n",
        "    'min_child_samples': 5# minimum number of data needed in a leaf\n",
        "}\n",
        "\n",
        "# Initiate classifier to use\n",
        "mdl = lgb.LGBMClassifier(boosting_type= 'dart', \n",
        "          objective = 'binary', \n",
        "          n_jobs = 5, \n",
        "          silent = True,\n",
        "          max_depth = params['max_depth'],\n",
        "          max_bin = params['max_bin'], \n",
        "          subsample_for_bin = params['subsample_for_bin'],\n",
        "          subsample = params['subsample'], \n",
        "          min_split_gain = params['min_split_gain'], \n",
        "          min_child_weight = params['min_child_weight'], \n",
        "          min_child_samples = params['min_child_samples'])\n",
        "\n",
        "# To view the default model parameters:\n",
        "mdl.get_params().keys()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "87u0UCW3ZSRZ"
      },
      "outputs": [],
      "source": [
        "#Grid search\n",
        "gridParams = {\n",
        "    'learning_rate': [0.005, 0.01],\n",
        "    'n_estimators': [8,16,24],\n",
        "    'num_leaves': [6,8,12,16], # large num_leaves helps improve accuracy but might lead to over-fitting\n",
        "    # 'boosting_type' : ['gbdt', 'dart'], # for better accuracy -> try dart\n",
        "    'objective' : ['binary'],\n",
        "    'max_bin':[255, 510], # large max_bin helps improve accuracy but might slow down training progress\n",
        "    'random_state' : [500],\n",
        "    # 'colsample_bytree' : [0.64, 0.65, 0.66],\n",
        "    # 'subsample' : [0.7,0.75],\n",
        "    # 'reg_alpha' : [1,1.2],\n",
        "    # 'reg_lambda' : [1,1.2,1.4],\n",
        "    }\n",
        "\n",
        "grid = GridSearchCV(mdl, gridParams, verbose=1, cv=3, n_jobs=5)\n",
        "# Run the grid\n",
        "grid.fit(X_res, y_res)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(grid.best_params_)\n",
        "print(grid.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jn8yoZUxaoV3"
      },
      "outputs": [],
      "source": [
        "# params['colsample_bytree'] = grid.best_params_['colsample_bytree']\n",
        "params['learning_rate'] = grid.best_params_['learning_rate'] \n",
        "params['max_bin'] = grid.best_params_['max_bin']\n",
        "params['num_leaves'] = grid.best_params_['num_leaves']\n",
        "params['n_estimators'] = grid.best_params_['n_estimators']\n",
        "# params['reg_alpha'] = grid.best_params_['reg_alpha']\n",
        "# params['reg_lambda'] = grid.best_params_['reg_lambda']\n",
        "# params['subsample'] = grid.best_params_['subsample']\n",
        "\n",
        "\n",
        "# X_test = np.array(test.drop(['id'], axis=1))\n",
        "# ids = test['id'].values\n",
        "\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_res, y_res, test_size=0.1, random_state = 42)\n",
        "    \n",
        "del X_res, y_res; gc.collect();\n",
        "\n",
        "d_train = lgb.Dataset(X_train, label=y_train)\n",
        "d_valid = lgb.Dataset(X_valid, label=y_valid) \n",
        "\n",
        "watchlist = [d_train, d_valid]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y9fN071Xb_N3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "model = lgb.train(params, train_set=d_train, num_boost_round=1000, valid_sets=watchlist, early_stopping_rounds=50, verbose_eval=4)\n",
        "\n",
        "p_valid = model.predict(X_valid)\n",
        "print('LGBM: ', roc_auc_score(y_valid, p_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "erhOkDJ7cgoe"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "lgbm_fpr, lgbm_tpr, lgbm_thresold = roc_curve(y_valid, p_valid)\n",
        "\n",
        "def graph_roc_curve_multiple(lgbm_fpr, lgbm_tpr):\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.title('ROC Curve \\n of LGBM', fontsize=18)\n",
        "    plt.plot(lgbm_fpr, lgbm_tpr, label='LGBM Classifier Score: {:.4f}'.format(roc_auc_score(y_valid, p_valid)))\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.axis([-0.01, 1, 0, 1])\n",
        "    plt.xlabel('False Positive Rate', fontsize=16)\n",
        "    plt.ylabel('True Positive Rate', fontsize=16)\n",
        "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
        "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
        "                )\n",
        "    plt.legend()\n",
        "    \n",
        "graph_roc_curve_multiple(lgbm_fpr, lgbm_tpr)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6NHTeIg4OxCH"
      },
      "outputs": [],
      "source": [
        "# clf_ex=lightgbm.LGBMRegressor(n_estimators = 200)\n",
        "# clf_ex.fit(X = X_train, y = y_train)\n",
        "# clf_ex.booster_.save_model('LGBMmode.txt')\n",
        "# pred_train = clf_ex.predict(X_train)\n",
        "# pred = clf_ex.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sS6riL-D9YAQ"
      },
      "outputs": [],
      "source": [
        "# submission\n",
        "submission = pd.DataFrame({'id':X_test['loan_id'], 'is_default':pred})\n",
        "submission.to_csv('submission.csv', index = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7Z7n30R_piS9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import roc_auc_score\n",
        "print('LGBM: ', roc_auc_score(y_train, pred_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9YSDrmqQUPV8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "lgbm_fpr, lgbm_tpr, lgbm_thresold = roc_curve(y_train, pred_train)\n",
        "\n",
        "def graph_roc_curve_multiple(lgbm_fpr, lgbm_tpr):\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.title('ROC Curve \\n of LGBM', fontsize=18)\n",
        "    plt.plot(lgbm_fpr, lgbm_tpr, label='LGBM Classifier Score: {:.4f}'.format(roc_auc_score(y_train, pred_train)))\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.axis([-0.01, 1, 0, 1])\n",
        "    plt.xlabel('False Positive Rate', fontsize=16)\n",
        "    plt.ylabel('True Positive Rate', fontsize=16)\n",
        "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
        "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
        "                )\n",
        "    plt.legend()\n",
        "    \n",
        "graph_roc_curve_multiple(lgbm_fpr, lgbm_tpr)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7tAOVRYTgfE9"
      },
      "outputs": [],
      "source": [
        "# submission\n",
        "submission = pd.DataFrame({'id':test['loan_id'], 'is_default':pred})\n",
        "submission.to_csv('submission.csv', index = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ha3rL4XvvH9n"
      },
      "outputs": [],
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# This is explicitly used for undersampling.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
        "X_train = X_train.values\n",
        "X_test = X_test.values\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values\n",
        "\n",
        "classifiers = {\n",
        "    \"LogisiticRegression\": LogisticRegression(),\n",
        "    # \"KNearest\": KNeighborsClassifier(),\n",
        "    # \"Support Vector Classifier\": SVC(),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
        "}\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "for key, classifier in classifiers.items():\n",
        "    classifier.fit(X_train, y_train)\n",
        "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
        "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fuQaIQ3oy3fe"
      },
      "outputs": [],
      "source": [
        "# Use GridSearchCV to find the best parameters.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# Logistic Regression \n",
        "# log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "\n",
        "\n",
        "\n",
        "# grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n",
        "# grid_log_reg.fit(X_train, y_train)\n",
        "# We automatically get the logistic regression with the best parameters.\n",
        "# log_reg = grid_log_reg.best_estimator_\n",
        "\n",
        "# knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
        "\n",
        "# grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\n",
        "# grid_knears.fit(X_train, y_train)\n",
        "# # KNears best estimator\n",
        "# knears_neighbors = grid_knears.best_estimator_\n",
        "\n",
        "# # Support Vector Classifier\n",
        "# svc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
        "# grid_svc = GridSearchCV(SVC(), svc_params)\n",
        "# grid_svc.fit(X_train, y_train)\n",
        "\n",
        "# # SVC best estimator\n",
        "# svc = grid_svc.best_estimator_\n",
        "\n",
        "# DecisionTree Classifier\n",
        "tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n",
        "              \"min_samples_leaf\": list(range(5,7,1))}\n",
        "grid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\n",
        "grid_tree.fit(X_train, y_train)\n",
        "\n",
        "# tree best estimator\n",
        "tree_clf = grid_tree.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8DkytRGQNv_u"
      },
      "outputs": [],
      "source": [
        "tree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\n",
        "print('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YZX5w_erzCSj"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# Create a DataFrame with all the scores and the classifiers names.\n",
        "\n",
        "log_reg_pred = cross_val_predict(log_reg, X_train, y_train.ravel(), cv=5, method=\"decision_function\")\n",
        "\n",
        "# knears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n",
        "\n",
        "# svc_pred = cross_val_predict(svc, X_train, y_train.ravel(), cv=5,\n",
        "#                              method=\"decision_function\")\n",
        "tree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2-Exl9yCzIeF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\n",
        "# print('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\n",
        "# print('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\n",
        "print('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BL96Q8ofzNGN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\n",
        "# knear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\n",
        "# svc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\n",
        "tree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n",
        "\n",
        "\n",
        "def graph_roc_curve_multiple(log_fpr, log_tpr, tree_fpr, tree_tpr):\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.title('ROC Curve \\n Top 3 Classifiers', fontsize=18)\n",
        "    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n",
        "    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.axis([-0.01, 1, 0, 1])\n",
        "    plt.xlabel('False Positive Rate', fontsize=16)\n",
        "    plt.ylabel('True Positive Rate', fontsize=16)\n",
        "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
        "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
        "                )\n",
        "    plt.legend()\n",
        "    \n",
        "graph_roc_curve_multiple(log_fpr, log_tpr, tree_fpr, tree_tpr)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YpFmq6rZMCB5"
      },
      "outputs": [],
      "source": [
        "# submission\n",
        "submission = pd.DataFrame({'id':test['loan_id'], 'is_default':pred})\n",
        "submission.to_csv('submission.csv', index = None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9n2mrj6gfE9"
      },
      "source": [
        "#### NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tZNXh-J8gfE-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Dropout , Lambda, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import  backend as K\n",
        "from keras import models\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sH5gqLDZgfE_"
      },
      "outputs": [],
      "source": [
        "# 数据标准化\n",
        "import numpy as np\n",
        "train1 = train1_data.drop(['earlies_credit_mon','loan_id','user_id'], axis = 1, inplace = False)\n",
        "# y_train1 = train_bank[['loan_id','isDefault']]\n",
        "\n",
        "train2 = train2_data.drop(['earlies_credit_mon','loan_id','user_id'], axis = 1, inplace = False)\n",
        "# y_train2 = train_internet[['loan_id','isdefault']]\n",
        "# X_train = pd.concat([X_train1, X_train2])\n",
        "# y_train = pd.concat([y_train1, y_train2])\n",
        "total_data=pd.concat([train1,train2])\n",
        "total_data = total_data.dropna()\n",
        "default_df = total_data.loc[total_data['isDefault'] == 1]\n",
        "ndefault_df = total_data.loc[total_data['isDefault'] == 0][:137888]  ## 555177 not default columns and 137888 default columns\n",
        "resample_df = pd.concat([default_df, ndefault_df])\n",
        "\n",
        "# Shuffle dataframe rows\n",
        "new_df = resample_df.sample(frac=1, random_state=42)\n",
        "X_res = new_df.drop(['isDefault'], axis = 1, inplace = False)\n",
        "y_res = new_df['isDefault']\n",
        "\n",
        "X_res = total_data.drop(['isDefault'], axis = 1, inplace = False)\n",
        "y_res = total_data['isDefault']\n",
        "X_test = test_data.drop(['earlies_credit_mon','loan_id','user_id'], axis = 1, inplace = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "60nocgrOgfE_"
      },
      "outputs": [],
      "source": [
        "# 缺失值填补\n",
        "X_res.fillna(0, inplace = True)\n",
        "X_test.fillna(0, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DTjkuxn3WiaS"
      },
      "outputs": [],
      "source": [
        "X_train = X_res.to_numpy()\n",
        "X_test=X_test.to_numpy()\n",
        "# mean_px = X_res.mean().astype(np.float32)\n",
        "mean_px = X_train.mean(axis=0)\n",
        "# std_px = X_res.std().astype(np.float32)\n",
        "std_px = X_train.std(axis=0)\n",
        "def standardize(x): \n",
        "    return (x-mean_px)/std_px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t2h5NDZZgfFA"
      },
      "outputs": [],
      "source": [
        "X_train_NN =(X_train - mean_px) / std_px\n",
        "X_test_NN  = (X_test - mean_px) / std_px\n",
        "\n",
        "X_train_NN = (X_res.values).astype('float32') # all pixel values\n",
        "y_train_NN = y_res\n",
        "\n",
        "X_test_NN = X_test.astype('float32') # all pixel values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_h_JWcDCgfFB"
      },
      "outputs": [],
      "source": [
        "# 修改初始化、加归一层、加dropout、改用不同的metrics\n",
        "seed = 43\n",
        "np.random.seed(seed)\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.metrics import AUC\n",
        "\n",
        "def auroc(y_true, y_pred):\n",
        "    return tf.compat.v1.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
        "\n",
        "input_shape = X_train_NN.shape[1]\n",
        "b_size = 2000\n",
        "max_epochs = 20\n",
        "\n",
        "import tensorflow.keras as K\n",
        "init = K.initializers.glorot_uniform(seed=1)\n",
        "simple_adam = K.optimizers.Adam(lr=0.001)\n",
        "\n",
        "model = K.models.Sequential()\n",
        "model.add(K.layers.Dense(units=256, input_dim=input_shape, kernel_initializer='he_normal', activation='relu',kernel_regularizer=l2(0.0001)))\n",
        "model.add(K.layers.LayerNormalization())\n",
        "model.add(K.layers.Dropout(0.3))\n",
        "model.add(K.layers.Dense(units= 64, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(K.layers.LayerNormalization())\n",
        "model.add(K.layers.Dropout(0.3))\n",
        "model.add(K.layers.Dense(units=1, kernel_initializer='he_normal', activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer=simple_adam, metrics=['accuracy',AUC(name='auc')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xSvr8Z7dgfFC"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HBAPqjp1gfFC",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"Starting NN training\")\n",
        "h = model.fit(X_train_NN, y_train_NN, batch_size=b_size, epochs=max_epochs, shuffle=True, verbose=1)\n",
        "print(\"NN training finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "77ALMXk-gfFD"
      },
      "outputs": [],
      "source": [
        "pred_NN = model.predict(X_test_NN)\n",
        "pred_NN = [item[0] for item in pred_NN]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9ts3Fy9KgfFE"
      },
      "outputs": [],
      "source": [
        "model.save('NN_model.h5')\n",
        "submission = pd.DataFrame({'id':test['loan_id'], 'is_default':pred_NN})\n",
        "submission.to_csv('submission.csv', index = None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUS6rQ9AgfFE"
      },
      "source": [
        "# 其他尝试..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4UQZu5r4Z9s"
      },
      "source": [
        "Pytorch NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iVoRa0f8Jyz"
      },
      "outputs": [],
      "source": [
        "# pytorch mlp for binary classification\n",
        "from numpy import vstack\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Sigmoid\n",
        "from torch.nn import Module\n",
        "from torch.optim import SGD\n",
        "from torch.nn import BCELoss\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.optim import optimizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler    \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f-KJ4YaHGl0",
        "outputId": "69f3c9bb-9da8-413d-b9e3-f19408616a0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5FcN9qpRxoz"
      },
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X_res, y_res, test_size=0.33, random_state=69)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYsz1sljSBPk"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfQNfgJQSe2Q"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50  ##100\n",
        "BATCH_SIZE = 100  ##change to 1000\n",
        "LEARNING_RATE = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTrwrBJwWDQt"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.Tensor(X_train)\n",
        "y_train_tensor = torch.Tensor(y_train.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6U3dZqeSOUw"
      },
      "outputs": [],
      "source": [
        "## train data\n",
        "class trainData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "\n",
        "\n",
        "train_data = trainData(torch.FloatTensor(X_train), torch.FloatTensor(y_train.values))\n",
        "## test data    \n",
        "class testData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data):\n",
        "        self.X_data = X_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "    \n",
        "\n",
        "test_data = testData(torch.FloatTensor(X_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R88CYCgsSVbo"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KCmDCtERbWx"
      },
      "outputs": [],
      "source": [
        "class binaryClassification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(binaryClassification, self).__init__()\n",
        "        # Number of input features is 12.\n",
        "        self.layer_1 = nn.Linear(34, 64) \n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.layer_out = nn.Linear(64, 1) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RLRXgjESr5u",
        "outputId": "4adfbd81-220d-4ce1-913b-91a851642b7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=34, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = binaryClassification()\n",
        "model.to(device)\n",
        "print(model)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "###################### OUTPUT ######################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBowLNDwS3Oi"
      },
      "outputs": [],
      "source": [
        "def binary_acc(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "    \n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AELdoRUETC2W",
        "outputId": "f4a90ae0-0e99-4882-813f-3e5f4965d98b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001: | Loss: 0.42281 | Acc: 80.707\n",
            "Epoch 002: | Loss: 0.40558 | Acc: 81.368\n",
            "Epoch 003: | Loss: 0.40391 | Acc: 81.470\n",
            "Epoch 004: | Loss: 0.40289 | Acc: 81.492\n",
            "Epoch 005: | Loss: 0.40199 | Acc: 81.571\n",
            "Epoch 006: | Loss: 0.40132 | Acc: 81.561\n",
            "Epoch 007: | Loss: 0.40106 | Acc: 81.575\n",
            "Epoch 008: | Loss: 0.40056 | Acc: 81.606\n",
            "Epoch 009: | Loss: 0.40019 | Acc: 81.610\n",
            "Epoch 010: | Loss: 0.40002 | Acc: 81.677\n",
            "Epoch 011: | Loss: 0.39960 | Acc: 81.655\n",
            "Epoch 012: | Loss: 0.39932 | Acc: 81.671\n",
            "Epoch 013: | Loss: 0.39930 | Acc: 81.683\n",
            "Epoch 014: | Loss: 0.39914 | Acc: 81.716\n",
            "Epoch 015: | Loss: 0.39887 | Acc: 81.733\n",
            "Epoch 016: | Loss: 0.39869 | Acc: 81.714\n",
            "Epoch 017: | Loss: 0.39857 | Acc: 81.699\n",
            "Epoch 018: | Loss: 0.39856 | Acc: 81.712\n",
            "Epoch 019: | Loss: 0.39842 | Acc: 81.701\n",
            "Epoch 020: | Loss: 0.39829 | Acc: 81.726\n",
            "Epoch 021: | Loss: 0.39808 | Acc: 81.727\n",
            "Epoch 022: | Loss: 0.39800 | Acc: 81.726\n",
            "Epoch 023: | Loss: 0.39767 | Acc: 81.705\n",
            "Epoch 024: | Loss: 0.39782 | Acc: 81.707\n",
            "Epoch 025: | Loss: 0.39769 | Acc: 81.698\n",
            "Epoch 026: | Loss: 0.39748 | Acc: 81.743\n",
            "Epoch 027: | Loss: 0.39760 | Acc: 81.718\n",
            "Epoch 028: | Loss: 0.39735 | Acc: 81.730\n",
            "Epoch 029: | Loss: 0.39732 | Acc: 81.748\n",
            "Epoch 030: | Loss: 0.39730 | Acc: 81.727\n",
            "Epoch 031: | Loss: 0.39723 | Acc: 81.719\n",
            "Epoch 032: | Loss: 0.39722 | Acc: 81.778\n",
            "Epoch 033: | Loss: 0.39727 | Acc: 81.741\n",
            "Epoch 034: | Loss: 0.39689 | Acc: 81.768\n",
            "Epoch 035: | Loss: 0.39682 | Acc: 81.781\n",
            "Epoch 036: | Loss: 0.39666 | Acc: 81.770\n",
            "Epoch 037: | Loss: 0.39686 | Acc: 81.779\n",
            "Epoch 038: | Loss: 0.39689 | Acc: 81.724\n",
            "Epoch 039: | Loss: 0.39668 | Acc: 81.763\n",
            "Epoch 040: | Loss: 0.39646 | Acc: 81.752\n",
            "Epoch 041: | Loss: 0.39642 | Acc: 81.745\n",
            "Epoch 042: | Loss: 0.39654 | Acc: 81.778\n",
            "Epoch 043: | Loss: 0.39643 | Acc: 81.802\n",
            "Epoch 044: | Loss: 0.39627 | Acc: 81.789\n",
            "Epoch 045: | Loss: 0.39641 | Acc: 81.776\n",
            "Epoch 046: | Loss: 0.39594 | Acc: 81.805\n",
            "Epoch 047: | Loss: 0.39634 | Acc: 81.792\n",
            "Epoch 048: | Loss: 0.39618 | Acc: 81.784\n",
            "Epoch 049: | Loss: 0.39636 | Acc: 81.786\n",
            "Epoch 050: | Loss: 0.39618 | Acc: 81.786\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "for e in range(1, EPOCHS+1):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_batch)\n",
        "        \n",
        "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
        "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "\n",
        "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI8SdoXPTeNJ"
      },
      "outputs": [],
      "source": [
        "y_pred_list = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_test_pred = model(X_batch)\n",
        "        y_test_pred = torch.sigmoid(y_test_pred)\n",
        "        y_pred_tag = torch.round(y_test_pred)\n",
        "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
        "\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2WGkdgfY8uY",
        "outputId": "5acd7cbc-25ff-47f1-c738-0257ca923480"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[176196,   6914],\n",
              "       [ 35077,  10525]])"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_valid, y_pred_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7WJGQH5ZO_B",
        "outputId": "57052a7e-0995-4937-ec64-d3c4054bb70c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.96      0.89    183110\n",
            "           1       0.60      0.23      0.33     45602\n",
            "\n",
            "    accuracy                           0.82    228712\n",
            "   macro avg       0.72      0.60      0.61    228712\n",
            "weighted avg       0.79      0.82      0.78    228712\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_valid, y_pred_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgqwIa45Zr7Y"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "25voQojEgfEm",
        "J9n2mrj6gfE9"
      ],
      "name": "credit_baseline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}